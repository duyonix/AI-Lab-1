struct POMG
    Î³  # discount factor
    â„  # agents
    ğ’®  # state space
    ğ’œ  # joint action space
    ğ’ª  # joint observation space
    T  # transition function
    O  # joint observation function
    R  # joint reward function
end

function POMG(pomg::BabyPOMG)
    return POMG(
        pomg.babyPOMDP.Î³, # discount factor
        vec(collect(1:n_agents(pomg))), # agents
        ordered_states(pomg), # state
        [ordered_actions(pomg, i) for i in 1:n_agents(pomg)], # joint action space
        [ordered_observations(pomg, i) for i in 1:n_agents(pomg)], # joint observation space
        (s, a, sâ€²) -> transition(pomg, s, a, sâ€²),  # Transition(s'|s, a)
        (a, sâ€², o) -> joint_observation(pomg, a, sâ€², o), # joint observation function
        (s, a) -> joint_reward(pomg, s, a) # Reward(s, a)
    )
end

function lookahead(ğ’«::POMG, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s, a, sâ€²) * sum(O(a, sâ€², o) * U(o, sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s, a) + Î³ * uâ€²
end

function evaluate_plan(ğ’«::POMG, Ï€, s)
    # compute utility of conditional plan 
    a = Tuple(Ï€i() for Ï€i in Ï€)
    U(o, sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€, o)], sâ€²)
    return isempty(first(Ï€).subplans) ? ğ’«.R(s, a) : lookahead(ğ’«, U, s, a) # equation (26.1) page 528
end

function utility(ğ’«::POMG, b, Ï€)
    # compute utility of policy Ï€ from initial state distibution b
    u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
    return sum(bs * us for (bs, us) in zip(b, u)) # equation (26.2) page 528
end