include("./HexWorld.jl")

# Init  policy evaluation : ulity 
# Symbol Upi(s) for state s and action pi(s)/a
function lookahead(ğ’«::MDP, U::Vector{Float64}, s::Int64, a::Int64)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R[s, a] + Î³ * sum(T[s, a, sâ€²] * U[sâ€²] for sâ€² in ğ’®)
end

#Policy evaluation is done iteratively
#Symbol myPolicy with k_max iterations
function iterative_policy_evaluation(ğ’«::MDP, myPolicy, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, myPolicy(s)) for s in ğ’®]
    end
    return U
end

#Value Function Policy
struct ValueFunctionPolicy
    ğ’«::MDP # problem
    U::Vector{Float64} # utility function
end
#policy improvement
function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a -> lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a = a, u = u)
end
(myPolicy::ValueFunctionPolicy)(s) = greedy(myPolicy.ğ’«, myPolicy.U, s).a


#Policy iteration => compute an optimal policy 
#It involves iterating between policy evaluation and policy improvement 
function solve(ğ’«::MDP, myPolicy, k_max)
    ğ’® = ğ’«.ğ’®
    for k = 1:k_max
        U = iterative_policy_evaluation(ğ’«, myPolicy, k_max) #policy evaluation
        myPolicyâ€² = ValueFunctionPolicy(ğ’«, U) #policy improvement
        if all(myPolicy(s) == myPolicyâ€²(s) for s in ğ’®) #converge
            break
        end
        myPolicy = myPolicyâ€²

    end
    return myPolicy
end


