import Pkg

using JuMP
using LinearAlgebra

struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end

struct RockPaperScissors end

n_agents(simpleGame::RockPaperScissors) = 2

ordered_actions(simpleGame::RockPaperScissors, i::Int) = [:rock, :paper, :scissors]  # choose 1 in 3 actions
ordered_joint_actions(simpleGame::RockPaperScissors) = vec(collect(Iterators.product([ordered_actions(simpleGame, i) for i = 1:n_agents(simpleGame)]...))) # create joint action space

n_joint_actions(simpleGame::RockPaperScissors) = length(ordered_joint_actions(simpleGame)) # amount of joint action space
n_actions(simpleGame::RockPaperScissors, i::Int) = length(ordered_actions(simpleGame, i)) # amount of actions

function reward(simpleGame::RockPaperScissors, i::Int, a)
    if i == 1
        noti = 2
    else
        noti = 1
    end

    if a[i] == a[noti]  # when draw, both reward = 0 
        r = 0.0
        # 6 cases when 1 win, 1 lose
        # reward of winner: 1.0, loser: -1.0
    elseif a[i] == :rock && a[noti] == :paper
        r = -1.0
    elseif a[i] == :rock && a[noti] == :scissors
        r = 1.0
    elseif a[i] == :paper && a[noti] == :rock
        r = 1.0
    elseif a[i] == :paper && a[noti] == :scissors
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :rock
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :paper
        r = 1.0
    end

    return r
end

function joint_reward(simpleGame::RockPaperScissors, a)
    # return vector U, U[i] is utility of agent i with joint action a
    return [reward(simpleGame, i, a) for i = 1:n_agents(simpleGame)]
end

function SimpleGame(simpleGame::RockPaperScissors)
    return SimpleGame(
        0.9,
        vec(collect(1:n_agents(simpleGame))),
        [ordered_actions(simpleGame, i) for i = 1:n_agents(simpleGame)],
        (a) -> joint_reward(simpleGame, a)
    )
end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k, v) in zip(keys(p), vs))) # return SimpleGamePolicy from dictionary
    end

    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))  # return SimpleGamePolicy with probability of ai is 1
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)  # return probability agent i will do action ai

function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)  # return random action
end

joint(X) = vec(collect(Iterators.product(X...)))  # construct joint action space from X
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]  # replace Ï€(i) with Ï€i

function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i] * p(a) for a in joint(ğ’œ))  # the utility of agent i with joint policy Ï€
end

function best_response(ğ’«::SimpleGame, Ï€, i)
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    # loop value in ğ’«.ğ’œ[i] and find U[value] max
    ai = argmax(U, ğ’«.ğ’œ[i])
    return SimpleGamePolicy(ai)  # return deterministic best response with joint policy Ï€
end

function softmax_response(ğ’«::SimpleGame, Ï€, i, Î»)
    ğ’œi = ğ’«.ğ’œ[i]
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    return SimpleGamePolicy(ai => exp(Î» * U(ai)) for ai in ğ’œi)   # return softmax response, model how agent will select action ai
end

struct IteratedBestResponse
    k_max # number of iterations
    Ï€ # initial policy
end

# We use IteratedBestResponse because it MAY converge to Nash equilibrium
# Algorithm For Decision Making (page 495) 

function IteratedBestResponse(ğ’«::SimpleGame, k_max)
    # k_max: how many times play
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
    return IteratedBestResponse(k_max, Ï€)
end

function solve(M::IteratedBestResponse, ğ’«::SimpleGame)
    Ï€ = M.Ï€
    for k = 1:M.k_max # loop k_max times
        # find best response with every agents
        Ï€ = [best_response(ğ’«, Ï€, i) for i in ğ’«.â„]
    end
    return Ï€  # return policy (Nash equilibrium)
end


struct HierarchicalSoftmax
    Î» # precision parameter
    k # level
    Ï€ # initial policy
end

function HierarchicalSoftmax(ğ’«::SimpleGame, Î», k)
    # vá»›i má»—i joint action, vÃ o tá»«ng action rá»“i gÃ¡n dict value lÃ  1
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]  # level k=0 is choosing action randomly
    return HierarchicalSoftmax(Î», k, Ï€)
    # aims to model human agents, because people often do not play Nash equilibrium strategy
end

function solve(M::HierarchicalSoftmax, ğ’«)
    Ï€ = M.Ï€
    for k = 1:M.k
        Ï€ = [softmax_response(ğ’«, Ï€, i, M.Î») for i in ğ’«.â„]
        # level k is a softmax response of level k-1
    end
    return Ï€
end




mutable struct JointCorrelatedPolicy
    p # dictionary mapping from joint actions to probabilities
    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))
end
(Ï€::JointCorrelatedPolicy)(a) = get(Ï€.p, a, 0.0)
function (Ï€::JointCorrelatedPolicy)()
    D = SetCategorical(collect(keys(Ï€.p)), collect(values(Ï€.p)))
    return rand(D)
end


struct CorrelatedEquilibrium end
function solve(M::CorrelatedEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    model = Model(Ipopt.Optimizer)
    @variable(model, Ï€[joint(ğ’œ)] â‰¥ 0)
    @objective(model, Max, sum(sum(Ï€[a] * R(a) for a in joint(ğ’œ))))
    @constraint(model, [i = â„, ai = ğ’œ[i], aiâ€² = ğ’œ[i]],
        sum(R(a)[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai)
        â‰¥
        sum(R(joint(a, aiâ€², i))[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai))
    @constraint(model, sum(Ï€) == 1)
    optimize!(model)
    return JointCorrelatedPolicy(a => value(Ï€[a]) for a in joint(ğ’œ))
end

simpleGame = RockPaperScissors()
P = SimpleGame(simpleGame)



H = HierarchicalSoftmax(P, 0.3, 20) # H is used for finding policy for human agents
D = solve(H, P)

for i = 2:100
    print(i)
    print(": ")
    println(D[1].p[i])
end

