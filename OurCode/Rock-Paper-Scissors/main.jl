using Pkg

using LinearAlgebra
using GridInterpolations
using CategoricalArrays
using Distributions
using Random
using JuMP
using Ipopt
using StatsPlots
using DataFrames

# using IndexedTables
struct SimpleGame
    Œ≥  # discount factor
    ‚Ñê  # agents
    ùíú  # joint action space
    R  # joint reward function
end

struct RockPaperScissors end

# 2 agents
n_agents(simpleGame::RockPaperScissors) = 2
# 3 actions
ordered_actions(simpleGame::RockPaperScissors, i::Int) = [:rock, :paper, :scissors]
# t·∫°o m·∫£ng tuple 2 actions => joint action space
ordered_joint_actions(simpleGame::RockPaperScissors) = vec(collect(Iterators.product([ordered_actions(simpleGame, i) for i = 1:n_agents(simpleGame)]...)))
n_joint_actions(simpleGame::RockPaperScissors) = length(ordered_joint_actions(simpleGame))
n_actions(simpleGame::RockPaperScissors, i::Int) = length(ordered_actions(simpleGame, i))

function reward(simpleGame::RockPaperScissors, i::Int, a)
    if i == 1
        noti = 2
    else
        noti = 1
    end

    if a[i] == a[noti]
        r = 0.0
    elseif a[i] == :rock && a[noti] == :paper
        r = -1.0
    elseif a[i] == :rock && a[noti] == :scissors
        r = 1.0
    elseif a[i] == :paper && a[noti] == :rock
        r = 1.0
    elseif a[i] == :paper && a[noti] == :scissors
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :rock
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :paper
        r = 1.0
    end

    return r
end

# 2 reward of a joint action tuple
function joint_reward(simpleGame::RockPaperScissors, a)
    return [reward(simpleGame, i, a) for i = 1:n_agents(simpleGame)]
end
# Kh·ªüi t·∫°o Simple Game cho b√†i to√°n RockPaperScissors
function SimpleGame(simpleGame::RockPaperScissors)
    return SimpleGame(
        0.9,
        vec(collect(1:n_agents(simpleGame))),
        [ordered_actions(simpleGame, i) for i = 1:n_agents(simpleGame)],
        (a) -> joint_reward(simpleGame, a)
    )
end

# Policy l√† 1 dictionary action-probability
struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    # ƒê·ªÉ t·∫°o ra struct SimpleGamePolicy ch·ª©a n·ªôi dung l√† 1 dictionary
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        # tr·∫£ v·ªÅ SimpleGamePolicy t·ª´ dictionary, ƒë∆∞·ª£c t√≠nh l√† action-probability
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k, v) in zip(keys(p), vs)))
    end

    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))  # return SimpleGamePolicy v·ªõi probability c·ªßa action ai l√† 1.0
end

(œÄi::SimpleGamePolicy)(ai) = get(œÄi.p, ai, 0.0)  # return probability agent i s·∫Ω th·ª±c hi·ªán action ai

struct SetCategorical{S}
    elements::Vector{S} # Set elements (could be repeated)
    distr::Categorical # Categorical distribution over set elements

    # support th√™m m·∫•y method ƒë√°nh gi√° function
    # normalize: chuy·ªÉn v·ªÅ 0->1 gi·ªØ nguy√™n t·ªâ l·ªá

    # norm(arr, lo·∫°i 1|2): ƒëo k/c 2 ƒëi·ªÉm
    # lo·∫°i 1: t·ªïng tr·ªã tuyet ƒë·ªëi
    # lo·∫°i 2: norm(arr) cƒÉn (t·ªïng c√°c b√¨nh ph∆∞∆°ng)
    function SetCategorical(elements::AbstractVector{S}) where {S}
        print("SetCategorical\n")

        weights = ones(length(elements)) # khoi tao t·∫ßn s·ªë m·ªói c√°i l√† 1
        return new{S}(elements, Categorical(normalize(weights, 1))) # t√≠nh x√°c su·∫•t b·∫±ng nhau
    end


    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where {S}
        #print("SetCategorical with weights\n")
        ‚Ñì‚ÇÅ = norm(weights, 1)
        # tr∆∞·ªùng h·ª£p ko c√≥ element n√†o xu·∫•t hi·ªán (= 0 all)
        if ‚Ñì‚ÇÅ < 1e-6 || isinf(‚Ñì‚ÇÅ)
            return SetCategorical(elements) # l√∫c ƒë·∫ßu ch·∫°y
        end
        distr = Categorical(normalize(weights, 1))
        # nghƒ©: weight: t·∫ßn s·ªë => t√≠nh x√°c su·∫•t cho m·ªói elements
        return new{S}(elements, distr)
    end
end

# over load
Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]

Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]

function Distributions.pdf(D::SetCategorical, x)
    # zip: ƒë√≥ng g√≥i theo c·∫∑p
    # l·∫•y x√°c su·∫•t c·ªßa x trong elements, c√≤n l·∫°i = 0
    sum(e == x ? w : 0.0 for (e, w) in zip(D.elements, D.distr.p))
end

function (œÄi::SimpleGamePolicy)()
    #print("œÄi::SimpleGamePolicy\n")
    # t·ª´ 2 arr keys + values (t·∫ßn ssu·∫•t) => 2 arr keys + x√°c su·∫•t
    D = SetCategorical(collect(keys(œÄi.p)), collect(values(œÄi.p)))
    return rand(D)  # return random action
end

joint(X) = vec(collect(Iterators.product(X...)))  # t·∫°o joint action space t·ª´ X
joint(œÄ, œÄi, i) = [i == j ? œÄi : œÄj for (j, œÄj) in enumerate(œÄ)]  # thay th·∫ø œÄ[i] b·∫±ng œÄi trong œÄ

function utility(ùí´::SimpleGame, œÄ, i)
    #print("utility\n")
    ùíú, R = ùí´.ùíú, ùí´.R
    # X√°c su·∫•t x·∫£y ra action a 
    p(a) = prod(œÄj(aj) for (œÄj, aj) in zip(œÄ, a))
    # t√≠nh U: ƒë√°nh gi√° ƒë·ªô thi·∫øt th·ª±c c·ªßa policy c·ªßa th·∫±ng agent i
    # a: 2 reward c·ªßa 2 th·∫±ng
    return sum(R(a)[i] * p(a) for a in joint(ùíú))  # the utility of agent i with joint policy œÄ
end

function best_response(ùí´::SimpleGame, œÄ, i)
    #print("best_response\n")
    U(ai) = utility(ùí´, joint(œÄ, SimpleGamePolicy(ai), i), i)
    ai = argmax(U, ùí´.ùíú[i])
    return SimpleGamePolicy(ai)  # tr·∫£ v·ªÅ deterministic best response v·ªõi joint policy œÄ
end


# visualize
struct VisualizeRPS
    model
    policy

    function VisualizeRPS(k_max)
        model = [DataFrame(rock = zeros(k_max), paper = zeros(k_max), scissors = zeros(k_max)),
            DataFrame(rock = zeros(k_max), paper = zeros(k_max), scissors = zeros(k_max))]
        policy = [DataFrame(rock = zeros(k_max), paper = zeros(k_max), scissors = zeros(k_max)),
            DataFrame(rock = zeros(k_max), paper = zeros(k_max), scissors = zeros(k_max))]
        return new(model, policy)
    end
end



function simulate(ùí´::SimpleGame, œÄ, k_max)
    #œÄ = [SimpleGamePolicy(ai => 1.0 for ai in ùíúi) for ùíúi in ùí´.ùíú] 
    v = VisualizeRPS(k_max)
    # v√°n 1: model => 1/3
    for k = 1:k_max
        # return random action
        a = [œÄi() for œÄi in œÄ]
        # k=1: model=1/3 policy = a
        for œÄi in œÄ
            update!(œÄi, a, v, k)
        end
    end

    return v, œÄ
end

mutable struct FictitiousPlay
    ùí´ # simple game
    i # agent index
    N # array of action count dictionaries => 2 Dict, m·ªói Dict 3 actions t∆∞∆°ng ·ª©ng 3 counts => ra policy(k·∫øt qu·∫£)
    # l∆∞u c·ªßa ƒë·ªëi th·ªß ƒë·ªÉ ƒë∆∞a quy·∫øt ƒë·ªãnh ƒë√°nh g√¨ ti·∫øp theo
    œÄi # current policy => ch·ªâ l∆∞u 1 action ƒë∆∞·ª£c ch·ªçn hi·ªán t·∫°i
end
function FictitiousPlay(ùí´::SimpleGame, i)
    # m·∫£ng g·ªìm 2 Dict c·ªßa 2 agents
    # kh·ªüi t·∫°o s·ªë l·∫ßn ra m·ªói action l√† 1 (counts)
    N = [Dict(aj => 1 for aj in ùí´.ùíú[j]) for j in ùí´.‚Ñê]
    œÄi = SimpleGamePolicy(ai => 1.0 for ai in ùí´.ùíú[i])
    return FictitiousPlay(ùí´, i, N, œÄi)
end
(œÄi::FictitiousPlay)() = œÄi.œÄi()
(œÄi::FictitiousPlay)(ai) = œÄi.œÄi(ai)

function update!(œÄi::FictitiousPlay, a, v, iteration)
    N, ùí´, ‚Ñê, i = œÄi.N, œÄi.ùí´, œÄi.ùí´.‚Ñê, œÄi.i

    # h√†m gi√∫p t√≠nh to√°n ƒë∆∞·ª£c policy c·ªßa agent j
    p(j) = SimpleGamePolicy(aj => u / sum(values(N[j])) for (aj, u) in N[j])
    # update visualize policy
    v.policy[i][iteration, a[i]] = 1


    # update visualize model
    v.model[i][iteration, :rock] = p(i).p[:rock]
    v.model[i][iteration, :paper] = p(i).p[:paper]
    v.model[i][iteration, :scissors] = p(i).p[:scissors]
    # println("----- iteration: ",iteration)
    # display(v.policy)
    # display(v.model)
    # println()
    for (j, aj) in enumerate(a)
        N[j][aj] += 1 # agent j with action aj +=1 count
    end
    # display(p(1))
    # m·∫£ng 2 policy c·ªßa 2 th·∫±ng
    œÄ = [p(j) for j in ‚Ñê] # l·∫•y policy (action-probability) c·ªßa m√¨nh & opponent, policy ƒë∆∞·ª£c t√≠nh t·ª´ x√°c su·∫•t (ƒë∆∞·ª£c t√≠nh t·ª´ N), xong quƒÉng v√¥ best_response t√≠nh to√°n tr·∫£ v·ªÅ œÄi.œÄi l√† action ƒë∆∞·ª£c ch·ªçn (quy·∫øt ƒë·ªãnh)
    # c·∫≠p nh·∫≠t current policy
    œÄi.œÄi = best_response(ùí´, œÄ, i)

end


# -----------Ch·∫°y ch∆∞∆°ng tr√¨nh------------------------
# Kh·ªüi t·∫°o Simple Game RockPaperScissors P
simpleGame = RockPaperScissors()
P = SimpleGame(simpleGame)

# Kh·ªüi t·∫°o FictitiousPlay cho m·ªói agent
pi = [(FictitiousPlay(P, i)) for i in 1:2]

# iteration: 100
k_max = 100
v, s = simulate(P, pi, k_max)

# visualize
model1 = @df v.model[1] plot(1:k_max, [:rock :paper :scissors], colour = [:red :blue :green], xlabel = "iteration", title = "opponent model (agent 1)")
model2 = @df v.model[2] plot(1:k_max, [:rock :paper :scissors], colour = [:red :blue :green], title = "opponent model (agent 2)")
policy1 = @df v.policy[1] plot(1:k_max, [:rock :paper :scissors], colour = [:red :blue :green], legend = false, title = "policy agent 1")
policy2 = @df v.policy[2] plot(1:k_max, [:rock :paper :scissors], colour = [:red :blue :green], legend = false, xlabel = "iteration", title = "policy agent 2")

plot(model2, policy1, model1, policy2, layout = (2, 2), size = (900, 700), grid = :off, ylim = (-0.05, 1))
