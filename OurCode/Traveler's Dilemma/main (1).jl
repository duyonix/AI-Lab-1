import Pkg

using JuMP
using LinearAlgebra

struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end

struct Travelers end

n_agents(simpleGame::Travelers) = 2

ordered_actions(simpleGame::Travelers, i::Int) = 2:100  # each traveler has to choose 1 integer from 2 to 100
ordered_joint_actions(simpleGame::Travelers) = vec(collect(Iterators.product([ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)]...)))
# Vector of ordered actions

n_joint_actions(simpleGame::Travelers) = length(ordered_joint_actions(simpleGame))
n_actions(simpleGame::Travelers, i::Int) = length(ordered_actions(simpleGame, i))

function reward(simpleGame::Travelers, i::Int, a)
    if i == 1
        noti = 2
    else
        noti = 1
    end
    if a[i] == a[noti]
        r = a[i]
    elseif a[i] < a[noti]
        r = a[i] + 2
    else
        r = a[noti] - 2
    end
    return r
end

function joint_reward(simpleGame::Travelers, a)
    # return vector U, U[i] is utility of agent i with joint action a
    return [reward(simpleGame, i, a) for i in 1:n_agents(simpleGame)]
end

function SimpleGame(simpleGame::Travelers)
    return SimpleGame(
        0.9,
        vec(collect(1:n_agents(simpleGame))),
        [ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)],
        (a) -> joint_reward(simpleGame, a)
    )
end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs))) # return SimpleGamePolicy from dictionary
    end

    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))  # return SimpleGamePolicy with probability of ai is 1
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)  # return probability agent i will do action ai

function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)  # return random action
end
    
joint(X) = vec(collect(Iterators.product(X...)))  # construct joint action space from X
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]  # replace Ï€(i) with Ï€i

function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i]*p(a) for a in joint(ğ’œ))  # the utility of agent i with joint policy Ï€
end

function best_response(ğ’«::SimpleGame, Ï€, i)
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    ai = argmax(U, ğ’«.ğ’œ[i])
    return SimpleGamePolicy(ai)  # return deterministic best response with joint policy Ï€
end

function softmax_response(ğ’«::SimpleGame, Ï€, i, Î»)
    ğ’œi = ğ’«.ğ’œ[i]
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    return SimpleGamePolicy(ai => exp(Î»*U(ai)) for ai in ğ’œi)   # return softmax response, model how agent will select action ai
end

struct IteratedBestResponse
    k_max # number of iterations
    Ï€ # initial policy
end

# We use IteratedBestResponse because it MAY converge to Nash equilibrium
# Algorithm For Decision Making (page 495) 

function IteratedBestResponse(ğ’«::SimpleGame, k_max)
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
    return IteratedBestResponse(k_max, Ï€)
end
    
function solve(M::IteratedBestResponse, ğ’«::SimpleGame)
    Ï€ = M.Ï€
    for k in 1:M.k_max
        Ï€ = [best_response(ğ’«, Ï€, i) for i in ğ’«.â„]
    end
    return Ï€  # return policy (Nash equilibrium)
end

struct HierarchicalSoftmax
    Î» # precision parameter
    k # level
    Ï€ # initial policy
end
    
function HierarchicalSoftmax(ğ’«::SimpleGame, Î», k)
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]  # level k=0 is choosing action randomly
    return HierarchicalSoftmax(Î», k, Ï€)  
    # aims to model human agents, because people often do not play Nash equilibrium strategy
end

function solve(M::HierarchicalSoftmax, ğ’«)
    Ï€ = M.Ï€
    for k in 1:M.k
        Ï€ = [softmax_response(ğ’«, Ï€, i, M.Î») for i in ğ’«.â„]
        # level k is a softmax response of level k-1
    end
    return Ï€
end


simpleGame=Travelers()  # simpleGame::Travelers
P=SimpleGame(simpleGame) # P is a SimpleGame instance according to simpleGame
M=IteratedBestResponse(P, 100) # M is used for finding Nash Equilibrium
H=HierarchicalSoftmax(P, 0.3, 5) # H is used for finding policy for human agents
D=solve(H, P)

for i in 2:100
    print(i)
    print(": ")
    println(D[1].p[i])
end
