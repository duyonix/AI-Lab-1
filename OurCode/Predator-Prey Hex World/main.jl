using Pkg

using Distributions
struct MG
    Î³  # discount factor
    â„  # agents
    ğ’®  # state space
    ğ’œ  # joint action space
    T  # transition function
    R  # joint reward function
end


include("discrete_mdp.jl")
include("hexworld.jl")
include("simplegame.jl")
include("helper.jl")
include("mg.jl")
include("grid.jl")

using Random
using JuMP
using Distributions
using CategoricalArrays
using LinearAlgebra
using GridInterpolations
using DataFrames




struct PredatorPreyHexWorldMG
    hexes::Vector{Tuple{Int,Int}}
    hexWorldDiscreteMDP::DiscreteMDP
end

n_agents(mg::PredatorPreyHexWorldMG) = 2

ordered_states(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:length(mg.hexes)))
ordered_states(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_states(mg, i) for i in 1:n_agents(mg)]...)))

ordered_actions(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:n_actions(mg.hexWorldDiscreteMDP)))
ordered_joint_actions(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_actions(mg, i) for i in 1:n_agents(mg)]...)))

n_actions(mg::PredatorPreyHexWorldMG, i::Int) = length(ordered_actions(mg, i))
n_joint_actions(mg::PredatorPreyHexWorldMG) = length(ordered_joint_actions(mg))

function transition(mg::PredatorPreyHexWorldMG, s, a, sâ€²)

    # Khi prey bá»‹ báº¯t, prey má»›i sáº½ Ä‘Æ°á»£c sinh ra, rá»“i dá»‹ch chuyá»ƒn ngáº«u nhiÃªn tá»›i má»™t vá»‹ trÃ­ nÃ o Ä‘Ã³ trÃªn hex map nÃªn tá»‰ lá»‡ sáº½ lÃ  1/12. CÃ²n predator sáº½ Ä‘á»©ng yÃªn
    if s[1] == s[2]
        prob = Float64(sâ€²[1] == s[1]) / length(mg.hexes)
        #display(prob)
    else


        # display(mg.hexWorldDiscreteMDP.T[:, :, 1])

        # NgÆ°á»£c láº¡i, transition cáº£ 2 sáº½ theo HexWorld
        # VÃ¬ cá»‘ 2 agents nÃªn nhÃ¢n láº¡i
        prob = mg.hexWorldDiscreteMDP.T[s[1], a[1], sâ€²[1]] * mg.hexWorldDiscreteMDP.T[s[2], a[2], sâ€²[2]]
    end
    # xÃ¡c suáº¥t transition cá»§a cáº£ 2 agents
    return prob
end

function reward(mg::PredatorPreyHexWorldMG, i::Int, s,a)
    r = 0.0

    if i == 1
        # Predators get -1 for moving and 10 for catching the prey.
        if s[1] == s[2]
            return 10.0
        else
            return -1.0
        end
    elseif i == 2
        # Prey get -1 for moving and -100 for being caught.
        if s[1] == s[2]
            r = -100.0
        else
            r = -1.0
        end
    end

    return r
end

function joint_reward(mg::PredatorPreyHexWorldMG, s, a)
    return [reward(mg, i, s, a) for i in 1:n_agents(mg)]
end

function MG(mg::PredatorPreyHexWorldMG)
    return MG(
        mg.hexWorldDiscreteMDP.Î³,
        vec(collect(1:n_agents(mg))),
        ordered_states(mg),
        [ordered_actions(mg, i) for i in 1:n_agents(mg)],
        (s, a, sâ€²) -> transition(mg, s, a, sâ€²),
        (s,a) -> joint_reward(mg, s,a)
    )
end

function PredatorPreyHexWorldMG(hexes::Vector{Tuple{Int,Int}},
    r_bump_border::Float64,
    p_intended::Float64,
    Î³::Float64)
    hexWorld = HexWorldMDP(hexes,
        r_bump_border,
        p_intended,
        Dict{Tuple{Int64,Int64},Float64}(),
        Î³)
    mdp = hexWorld.mdp
    return PredatorPreyHexWorldMG(hexes, mdp)
end

struct VisualizePPHW
    model
    policy
    states
    rewards
    function VisualizePPHW(k_max)
        k_max+=1
        model = [DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max)),
        DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max))]
        policy= [DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max)),
        DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max))]
        states=Vector{Tuple{Int64, Int64}}()
        rewards=Vector{Tuple{Int64, Int64}}()
        push!(rewards,(0,0))
        return new(model,policy,states,rewards)
    end
end

# const HexWorldRBumpBorder = -1.0 # Reward for falling off hex map
# const HexWorldPIntended = 0.7 # Probability of going intended direction
# const HexWorldDiscountFactor = 0.9
function PredatorPreyHexWorld()
    PredatorPreyHexWorld = PredatorPreyHexWorldMG(
        [
            (-1, 2), (0, 2), (1, 2),
            (-1, 1), (1, 1), (3, 1), (4, 1),
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
        ],
        HexWorldRBumpBorder,
        HexWorldPIntended,
        HexWorldDiscountFactor
    )
    return PredatorPreyHexWorld
end




struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

# á»Ÿ ulatr (Ï€i::SimpleGamePolicy)(ai)
(Ï€i::MGPolicy)(s, ai) = Ï€i.p[s](ai)
(Ï€i::SimpleGamePolicy)(s, ai) = Ï€i(ai)

probability(ğ’«::MG, s, Ï€, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
reward(ğ’«::MG, s, Ï€, i) =
    sum(ğ’«.R(s, a)[i] * probability(ğ’«, s, Ï€, a) for a in joint(ğ’«.ğ’œ))
transition(ğ’«::MG, s, Ï€, sâ€²) =
    sum(ğ’«.T(s, a, sâ€²) * probability(ğ’«, s, Ï€, a) for a in joint(ğ’«.ğ’œ))



mutable struct MGFictitiousPlay
    ğ’« # Markov game
    i # agent index
    Qi # state-action value estimates
    Ni # state-action counts
end
function MGFictitiousPlay(ğ’«::MG, i)
    â„, ğ’®, ğ’œ, R = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    # Qi= [(state 12, action 12)  => reward]
    Qi = Dict((s, a) => R(s, a)[i] for s in ğ’® for a in joint(ğ’œ))

    # display(Qi)
    # println("----------------")
    # A[j]: [1...6]

    # N[i]= [agent 1, state 12, action chá»n 1 trong [1:6]]
    Ni = Dict((j, s, aj) => 1.0 for j in â„ for s in ğ’® for aj in ğ’œ[j])
    # Î³  # discount factor
    # â„  # agents
    # ğ’®  # state space
    # ğ’œ  # joint action space
    # T  # transition function
    # R  # joint reward function 
    # (agent, in (12,12), action) => 1
    # display(Ni)
    return MGFictitiousPlay(ğ’«, i, Qi, Ni)
end

function (Ï€i::MGFictitiousPlay)(s,v,iteration)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³


    # SimpleGamePolicy(Dict(1=> count Ni,...6))
    # return Dict(6 keys Ni => xÃ¡c suáº¥t count)
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ğ’œ[i])

    # MGPolicy(Dict{Tuple{Int64, Int64}, SimpleGamePolicy}
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ğ’®)



    Ï€ = [Ï€iâ€²(i) for i in â„]
    # display(MGPolicy(s => Ï€iâ€²(i, s) for s in [(1,2),(2,3)]))
    # chÆ°a Ä‘á»c 
    # display(Ï€[1].p[(1,2)](4))

    # probability(ğ’«, s, Ï€, a): xÃ¡c suáº¥t táº¡i s12 chá»n a12

    # sum(reward*probability tÆ°Æ¡ng á»©ng)
    # U: Ä‘Ã¡nh giÃ¡ state s
    U(s, Ï€) = sum(Ï€i.Qi[s, a] * probability(ğ’«, s, Ï€, a) for a in joint(ğ’œ))

    # println(U((1,2),Ï€))


    # action value function : Q-funtion: 7.12
    # transition(ğ’«, s, Ï€, sâ€²): tá»•ng kháº£ nÄƒng transition tá»« s qua s' vá»›i má»i a cÃ³ thá»ƒ

    # Q: Ä‘Ã¡nh giÃ¡ state s trÃªn toÃ n bá»™ hex world
    Q(s, Ï€) = reward(ğ’«, s, Ï€, i) + Î³ * sum(transition(ğ’«, s, Ï€, sâ€²) * U(sâ€², Ï€)
                                           for sâ€² in ğ’®)

    # joint => [SimpleGamePolicy,MGPolicy ]; [MGPolicy,SimpleGamePolicy]
    # SimpleGamePolicy (ai => 1.0), aj  => 0

    # tÃ¬m action cho agent i => SimpleGamePolicy => tÃ­nh Q(ai) => quyáº¿n Ä‘á»‹nh chá»n ai 
    # Ä‘á»‘i thá»§ => MGPolicy => tÃ­nh háº¿t A
    Q(ai) = Q(s, joint(Ï€, SimpleGamePolicy(ai), i))
    # index cá»§a max element
    # x=[Q(i) for i in collect(1:6)]
    # println(x)
    # Q lÃ  function tÃ­nh toÃ¡n giÃ¡ trá»‹ Q-value cho tá»«ng elements in [1...6]

    # Ä‘i theo action nÃ o thÃ¬ kháº£ nÄƒng nháº­n dc reward cao nháº¥t
    ai = argmax(Q, ğ’«.ğ’œ[Ï€i.i])

    # println(ai)
    # display(SimpleGamePolicy(ai))

    # return Dict(): action nÃ o cÃ³ Q-value lá»›n nháº¥t => xs =1
    return SimpleGamePolicy(ai)
end
function update!(Ï€i::MGFictitiousPlay, s, a, sâ€²)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³



    # +1: tÃ­nh U => Q => ai
    for (j, aj) in enumerate(a)
        Ï€i.Ni[j, s, aj] += 1
    end

    # action => xÃ¡c suáº¥t count
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ğ’œ[i])
    # MGPolicy(Dict{Tuple{Int64, Int64}, SimpleGamePolicy}
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ğ’®)

    Ï€ = [Ï€iâ€²(i) for i in â„]

    U(Ï€, s) = sum(Ï€i.Qi[s, a] * probability(ğ’«, s, Ï€, a) for a in joint(ğ’œ))

    Q(s, a) = R(s, a)[i] + Î³ * sum(T(s, a, sâ€²) * U(Ï€, sâ€²) for sâ€² in ğ’®)

    # Ä‘Ã¡nh giÃ¡ trÃªn toÃ n bá»™ actions
    # vd: action 1, state s tá»›i nhá»¯ng state khÃ¡c s sáº½ 
    for a in joint(ğ’œ)
        # Cáº­p nháº­t láº¡i Q value cho reward cá»§a key (s,a) tÆ°Æ¡ng á»©ng
        Ï€i.Qi[s, a] = Q(s, a)
    end
end



function randstep(ğ’«::MG, s, a)
    # random dá»±a trÃªn phÃ¢n phá»‘i chuáº©n // tráº£ vá» state s' [base on propability á»Ÿ giá»¯a]
    # kháº£ nÄƒng xuÃ¡t hiá»‡n cao
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    #display(ğ’«.T)
    # display(T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®)
    # display("\n")
    # #display(sâ€²)
    # r lÃ  cá»‘ Ä‘á»‹nh
    r = ğ’«.R(s, a)
    return sâ€², r
end





function simulate(ğ’«::MG, Ï€, k_max, b)
    v = VisualizePPHW(k_max)
    # random vá»‹ trÃ­ state cá»§a 2 agent
    s = rand(b)
    while s[1]==s[2]
        s = rand(b)
    end
    push!(v.states,s)
    # k_max: iteration
    for k = 1:k_max
        # println("s => ", s)
        # (): return key, key la action ai cua SimpleGamePolicy
        # a: (action cua 1, action cua 2)
        a = Tuple(Ï€i(s,v,k)() for Ï€i in Ï€)
        
        # update visualize


        

        # println("-----------  a => ", a)
        #display(a)
        #random state má»›i
        sâ€², r = randstep(ğ’«, s, a)
        # println(s," => ",sâ€²)
        for Ï€i in Ï€
            # update láº¡i policy
            update!(Ï€i, s, a, sâ€²)
        end
        
        # update reward visualize
        
        if(s[1] != s[2])
            if(s[1]==sâ€²[1])
                r[1] = 0
            end
            if(s[2]==sâ€²[2])
                r[2] = 0
            end
        end
        push!(v.rewards,Tuple(r))
        push!(v.states,sâ€²)

        # sá»­ dá»¥ng state nÃ y lÃ m s
        s = sâ€²
        # update visualize


    end
    return v,Ï€
end




p = PredatorPreyHexWorld()
# display(p)
mg = MG(p)
Ï€ = [MGFictitiousPlay(mg, i) for i in 1:2]
#display(Ï€)
print("version ----------------------------------------\n\n\n\n\n")
k_max=10
v,policy=simulate(mg, Ï€, k_max, mg.ğ’®)


drawPredatorPreyHW(v.states,v.rewards,k_max)


