using Pkg

using Distributions
struct MG
    Œ≥  # discount factor
    ‚Ñê  # agents
    ùíÆ  # state space
    ùíú  # joint action space
    T  # transition function
    R  # joint reward function
end


include("discrete_mdp.jl")
include("hexworld.jl")
include("simplegame.jl")
include("helper.jl")
include("mg.jl")
include("grid.jl")

using Random
using JuMP
using Distributions
using CategoricalArrays
using LinearAlgebra
using GridInterpolations
using DataFrames
using StatsPlots
using IndexedTables



struct PredatorPreyHexWorldMG
    hexes::Vector{Tuple{Int,Int}}
    hexWorldDiscreteMDP::DiscreteMDP
end

n_agents(mg::PredatorPreyHexWorldMG) = 2

ordered_states(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:length(mg.hexes)))
ordered_states(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_states(mg, i) for i in 1:n_agents(mg)]...)))

ordered_actions(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:n_actions(mg.hexWorldDiscreteMDP)))
ordered_joint_actions(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_actions(mg, i) for i in 1:n_agents(mg)]...)))

n_actions(mg::PredatorPreyHexWorldMG, i::Int) = length(ordered_actions(mg, i))
n_joint_actions(mg::PredatorPreyHexWorldMG) = length(ordered_joint_actions(mg))

function transition(mg::PredatorPreyHexWorldMG, s, a, s‚Ä≤)

    # Khi prey b·ªã b·∫Øt, prey m·ªõi s·∫Ω ƒë∆∞·ª£c sinh ra, r·ªìi d·ªãch chuy·ªÉn ng·∫´u nhi√™n t·ªõi m·ªôt v·ªã tr√≠ n√†o ƒë√≥ tr√™n hex map n√™n t·ªâ l·ªá s·∫Ω l√† 1/12. C√≤n predator s·∫Ω ƒë·ª©ng y√™n
    if s[1] == s[2]
        prob = Float64(s‚Ä≤[1] == s[1]) / length(mg.hexes)
        #display(prob)
    else


        # display(mg.hexWorldDiscreteMDP.T[:, :, 1])

        # Ng∆∞·ª£c l·∫°i, transition c·∫£ 2 s·∫Ω theo HexWorld
        # V√¨ c·ªë 2 agents n√™n nh√¢n l·∫°i
        prob = mg.hexWorldDiscreteMDP.T[s[1], a[1], s‚Ä≤[1]] * mg.hexWorldDiscreteMDP.T[s[2], a[2], s‚Ä≤[2]]
    end
    # x√°c su·∫•t transition c·ªßa c·∫£ 2 agents
    return prob
end

function reward(mg::PredatorPreyHexWorldMG, i::Int, s,a)
    r = 0.0

    if i == 1
        # Predators get -1 for moving and 10 for catching the prey.
        if s[1] == s[2]
            return 10.0
        else
            return -1.0
        end
    elseif i == 2
        # Prey get -1 for moving and -100 for being caught.
        if s[1] == s[2]
            r = -100.0
        else
            r = -1.0
        end
    end

    return r
end

function joint_reward(mg::PredatorPreyHexWorldMG, s, a)
    return [reward(mg, i, s, a) for i in 1:n_agents(mg)]
end

function MG(mg::PredatorPreyHexWorldMG)
    return MG(
        mg.hexWorldDiscreteMDP.Œ≥,
        vec(collect(1:n_agents(mg))),
        ordered_states(mg),
        [ordered_actions(mg, i) for i in 1:n_agents(mg)],
        (s, a, s‚Ä≤) -> transition(mg, s, a, s‚Ä≤),
        (s,a) -> joint_reward(mg, s,a)
    )
end

function PredatorPreyHexWorldMG(hexes::Vector{Tuple{Int,Int}},
    r_bump_border::Float64,
    p_intended::Float64,
    Œ≥::Float64)
    hexWorld = HexWorldMDP(hexes,
        r_bump_border,
        p_intended,
        Dict{Tuple{Int64,Int64},Float64}(),
        Œ≥)
    mdp = hexWorld.mdp
    return PredatorPreyHexWorldMG(hexes, mdp)
end

struct VisualizePPHW
    model
    policy
    states
    rewards
    function VisualizePPHW(k_max)
        k_max+=1
        model = [DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max)),
        DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max))]
        policy= [DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max)),
        DataFrame(east=zeros(k_max),north_east=zeros(k_max),north_west=zeros(k_max),west=zeros(k_max),south_west=zeros(k_max),south_east=zeros(k_max))]
        states=Vector{Tuple{Int64, Int64}}()
        rewards=Vector{Tuple{Int64, Int64}}()

        push!(rewards,(0,0))
        model[1][1,:].=[1/6,1/6,1/6,1/6,1/6,1/6]
        model[2][1,:].=[1/6,1/6,1/6,1/6,1/6,1/6]
        
        return new(model,policy,states,rewards)
    end
end

# const HexWorldRBumpBorder = -1.0 # Reward for falling off hex map
# const HexWorldPIntended = 0.7 # Probability of going intended direction
# const HexWorldDiscountFactor = 0.9
function PredatorPreyHexWorld()
    PredatorPreyHexWorld = PredatorPreyHexWorldMG(
        [
            (-1, 2), (0, 2), (1, 2),
            (-1, 1), (1, 1), (3, 1), (4, 1),
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
        ],
        HexWorldRBumpBorder,
        HexWorldPIntended,
        HexWorldDiscountFactor
    )
    return PredatorPreyHexWorld
end




struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

# ·ªü ulatr (œÄi::SimpleGamePolicy)(ai)
(œÄi::MGPolicy)(s, ai) = œÄi.p[s](ai)
(œÄi::SimpleGamePolicy)(s, ai) = œÄi(ai)

probability(ùí´::MG, s, œÄ, a) = prod(œÄj(s, aj) for (œÄj, aj) in zip(œÄ, a))
reward(ùí´::MG, s, œÄ, i) =
    sum(ùí´.R(s, a)[i] * probability(ùí´, s, œÄ, a) for a in joint(ùí´.ùíú))
transition(ùí´::MG, s, œÄ, s‚Ä≤) =
    sum(ùí´.T(s, a, s‚Ä≤) * probability(ùí´, s, œÄ, a) for a in joint(ùí´.ùíú))



mutable struct MGFictitiousPlay
    ùí´ # Markov game
    i # agent index
    Qi # state-action value estimates
    Ni # state-action counts
end
function MGFictitiousPlay(ùí´::MG, i)
    ‚Ñê, ùíÆ, ùíú, R = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R
    # Qi= [(state 12, action 12)  => reward]
    Qi = Dict((s, a) => R(s, a)[i] for s in ùíÆ for a in joint(ùíú))

    # display(Qi)
    # println("----------------")
    # A[j]: [1...6]

    # N[i]= [agent 1, state 12, action ch·ªçn 1 trong [1:6]]
    Ni = Dict((j, s, aj) => 1.0 for j in ‚Ñê for s in ùíÆ for aj in ùíú[j])
    # Œ≥  # discount factor
    # ‚Ñê  # agents
    # ùíÆ  # state space
    # ùíú  # joint action space
    # T  # transition function 
    # R  # joint reward function 
    # (agent, in (12,12), action) => 1
    # display(Ni)
    return MGFictitiousPlay(ùí´, i, Qi, Ni)
end

function (œÄi::MGFictitiousPlay)(s)
    ùí´, i, Qi = œÄi.ùí´, œÄi.i, œÄi.Qi
    ‚Ñê, ùíÆ, ùíú, T, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥


    # SimpleGamePolicy(Dict(1=> count Ni,...6))
    # return Dict(6 keys Ni => x√°c su·∫•t count)
    œÄi‚Ä≤(i, s) = SimpleGamePolicy(ai => œÄi.Ni[i, s, ai] for ai in ùíú[i])

    # MGPolicy(Dict{Tuple{Int64, Int64}, SimpleGamePolicy}
    œÄi‚Ä≤(i) = MGPolicy(s => œÄi‚Ä≤(i, s) for s in ùíÆ)

    display(i)

    œÄ = [œÄi‚Ä≤(i) for i in ‚Ñê]
    # display(MGPolicy(s => œÄi‚Ä≤(i, s) for s in [(1,2),(2,3)]))
    # ch∆∞a ƒë·ªçc 
    # display(œÄ[1].p[(1,2)](4))

    # probability(ùí´, s, œÄ, a): x√°c su·∫•t t·∫°i s12 ch·ªçn a12

    # sum(reward*probability t∆∞∆°ng ·ª©ng)
    # U: ƒë√°nh gi√° state s
    U(s, œÄ) = sum(œÄi.Qi[s, a] * probability(ùí´, s, œÄ, a) for a in joint(ùíú))

    # println(U((1,2),œÄ)) 


    # action value function : Q-funtion: 7.12
    # transition(ùí´, s, œÄ, s‚Ä≤): t·ªïng kh·∫£ nƒÉng transition t·ª´ s qua s' v·ªõi m·ªçi a c√≥ th·ªÉ

    # Q: ƒë√°nh gi√° state s tr√™n to√†n b·ªô hex world
    Q(s, œÄ) = reward(ùí´, s, œÄ, i) + Œ≥ * sum(transition(ùí´, s, œÄ, s‚Ä≤) * U(s‚Ä≤, œÄ)
                                           for s‚Ä≤ in ùíÆ)

    # joint => [SimpleGamePolicy,MGPolicy ]; [MGPolicy,SimpleGamePolicy]
    # SimpleGamePolicy (ai => 1.0), aj  => 0

    # t√¨m action cho agent i => SimpleGamePolicy => t√≠nh Q(ai) => quy·∫øn ƒë·ªãnh ch·ªçn ai 
    # ƒë·ªëi th·ªß => MGPolicy => t√≠nh h·∫øt A
    Q(ai) = Q(s, joint(œÄ, SimpleGamePolicy(ai), i))
    # index c·ªßa max element
    # x=[Q(i) for i in collect(1:6)]
    # println(x)
    # Q l√† function t√≠nh to√°n gi√° tr·ªã Q-value cho t·ª´ng elements in [1...6]

    # ƒëi theo action n√†o th√¨ kh·∫£ nƒÉng nh·∫≠n dc reward cao nh·∫•t
    ai = argmax(Q, ùí´.ùíú[œÄi.i])

    display(ai)
    # display(SimpleGamePolicy(ai))

    # return Dict(): action n√†o c√≥ Q-value l·ªõn nh·∫•t => xs =1
    return SimpleGamePolicy(ai)
end
function update!(œÄi::MGFictitiousPlay, s, a, s‚Ä≤,v,iteration)
    ùí´, i, Qi = œÄi.ùí´, œÄi.i, œÄi.Qi
    ‚Ñê, ùíÆ, ùíú, T, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥

    # +1: t√≠nh U => Q => ai
    for (j, aj) in enumerate(a)
        œÄi.Ni[j, s, aj] += 1
    end

    # update visualize
    v.policy[i][iteration,a[i]]=1
    
    totalCount = sum(œÄi.Ni[i,S,ai] for S in ùíÆ for ai in ùíú[i])
    for ai in ùíú[i]
        v.model[i][iteration+1,ai]=sum(œÄi.Ni[i,S,ai] for S in ùíÆ)/totalCount
    end
    
    
    

    # action => x√°c su·∫•t count
    œÄi‚Ä≤(i, s) = SimpleGamePolicy(ai => œÄi.Ni[i, s, ai] for ai in ùíú[i])
    # MGPolicy(Dict{Tuple{Int64, Int64}, SimpleGamePolicy}
    œÄi‚Ä≤(i) = MGPolicy(s => œÄi‚Ä≤(i, s) for s in ùíÆ)

    œÄ = [œÄi‚Ä≤(i) for i in ‚Ñê]

    U(œÄ, s) = sum(œÄi.Qi[s, a] * probability(ùí´, s, œÄ, a) for a in joint(ùíú))

    Q(s, a) = R(s, a)[i] + Œ≥ * sum(T(s, a, s‚Ä≤) * U(œÄ, s‚Ä≤) for s‚Ä≤ in ùíÆ)

    # ƒë√°nh gi√° tr√™n to√†n b·ªô actions
    # vd: action 1, state s t·ªõi nh·ªØng state kh√°c s s·∫Ω 
    for a in joint(ùíú)
        # C·∫≠p nh·∫≠t l·∫°i Q value cho reward c·ªßa key (s,a) t∆∞∆°ng ·ª©ng
        œÄi.Qi[s, a] = Q(s, a)
    end
end



function randstep(ùí´::MG, s, a)
    # random d·ª±a tr√™n ph√¢n ph·ªëi chu·∫©n // tr·∫£ v·ªÅ state s' [base on propability ·ªü gi·ªØa]
    # kh·∫£ nƒÉng xu√°t hi·ªán cao
    s‚Ä≤ = rand(SetCategorical(ùí´.ùíÆ, [ùí´.T(s, a, s‚Ä≤) for s‚Ä≤ in ùí´.ùíÆ]))
    #display(ùí´.T)
    # display(T(s, a, s‚Ä≤) for s‚Ä≤ in ùí´.ùíÆ)
    # display("\n")
    # #display(s‚Ä≤)
    # r l√† c·ªë ƒë·ªãnh
    r = ùí´.R(s, a)
    return s‚Ä≤, r
end





function simulate(ùí´::MG, œÄ, k_max, b)
    v = VisualizePPHW(k_max)
    # random v·ªã tr√≠ state c·ªßa 2 agent
    s = rand(b)
    while s[1]==s[2]
        s = rand(b)
    end
    push!(v.states,s)
    # k_max: iteration
    for k = 1:k_max
        # println("s => ", s)
        # (): return key, key la action ai cua SimpleGamePolicy
        # a: (action cua 1, action cua 2)
        a = Tuple(œÄi(s)() for œÄi in œÄ)
        
        # println("-----------  a => ", a)
        display(a)
        #random state m·ªõi
        s‚Ä≤, r = randstep(ùí´, s, a)
        # println(s," => ",s‚Ä≤)
        for œÄi in œÄ
            # update l·∫°i policy
            update!(œÄi, s, a, s‚Ä≤,v,k)
        end
        
        # update reward visualize
        
        if(s[1] != s[2])
            if(s[1]==s‚Ä≤[1])
                r[1] = 0
            end
            if(s[2]==s‚Ä≤[2])
                r[2] = 0
            end
        end
        if(k > 1)
            r[1]+=v.rewards[k][1]
            r[2]+=v.rewards[k][2]
        end
        push!(v.rewards,Tuple(r))
        push!(v.states,s‚Ä≤)

        # s·ª≠ d·ª•ng state n√†y l√†m s
        s = s‚Ä≤

    end

# a d≈© helper
    a = Tuple(œÄi(s)() for œÄi in œÄ)
    for i in 1:2
        v.policy[i][k_max+1,a[i]]=1
    end

    return v,œÄ
end




p = PredatorPreyHexWorld()
# display(p)
mg = MG(p)
œÄ = [MGFictitiousPlay(mg, i) for i in 1:2]
#display(œÄ)
print("version ----------------------------------------\n\n\n\n\n")
k_max=10
v,policy=simulate(mg, œÄ, k_max, mg.ùíÆ)

# display(v)
# drawPredatorPreyHW(v.states,v.rewards,k_max)

# visualize

# colName = [:east :north_east :north_west :west :south_west :south_east]
model1= @df v.model[1] plot(0:k_max, [:east :north_east :north_west :west :south_west :south_east],legend=false, xlabel="iteration",title="opponent model - predator")
model2= @df v.model[2] plot(0:k_max, [:east :north_east :north_west :west :south_west :south_east],legend=false,title="opponent model - prey")
policy1= @df v.policy[1] plot(0:k_max,[:east :north_east :north_west :west :south_west :south_east],legend=false,title="policy - predator", framestyle = :none)
policy2= @df v.policy[2] plot(0:k_max, [:east :north_east :north_west :west :south_west :south_east],legend=false,xlabel="iteration",title="policy - prey")


plot(model2,policy1,model1,policy2,layout=(2,2),size=(1000,700), grid=:off)#,ylim=(-0.05,1))
