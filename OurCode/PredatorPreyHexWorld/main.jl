include("../HexWorld/DiscreteMDP.jl")
include("../HexWorld/HexWorld.jl")
include("../helpers/SimpleGame/SimpleGame.jl")
include("Visualize.jl")

using Random
using JuMP
using LinearAlgebra
using GridInterpolations
using DataFrames
using IndexedTables

# structure of Markov Game
struct MG
    Î³  # discount factor
    â„  # agents
    ð’®  # state space
    ð’œ  # joint action space
    T  # transition function
    R  # joint reward function
end

struct PredatorPreyHexWorldMG
    hexes::Vector{Tuple{Int,Int}}   # include coordinates of all hex cells
    hexWorldDiscreteMDP::DiscreteMDP
end

n_agents(mg::PredatorPreyHexWorldMG) = 2 # game include 2 agents: predator - prey

# state in order, from 1 to length(mg.hexes), of agent i
ordered_states(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:length(mg.hexes)))

# chá»‰nh há»£p táº­p state cá»§a cÃ¡c agents 
ordered_states(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_states(mg, i) for i in 1:n_agents(mg)]...)))

# action in order, from 1 to n_actions, of agent i
ordered_actions(mg::PredatorPreyHexWorldMG, i::Int) = vec(collect(1:n_actions(mg.hexWorldDiscreteMDP)))

# chá»‰nh há»£p táº­p action cá»§a cÃ¡c agents 
ordered_joint_actions(mg::PredatorPreyHexWorldMG) = vec(collect(Iterators.product([ordered_actions(mg, i) for i in 1:n_agents(mg)]...)))

# number of actions
n_actions(mg::PredatorPreyHexWorldMG, i::Int) = length(ordered_actions(mg, i))

# number of joint actions
n_joint_actions(mg::PredatorPreyHexWorldMG) = length(ordered_joint_actions(mg))


# calculate probability of transition from s to s' with action a of 2 agents
function transition(mg::PredatorPreyHexWorldMG, s, a, sâ€²)

    # when the prey is captured, it will be transposed to a random hex cell in hex map with probability = 1/12
    if s[1] == s[2]
        prob = Float64(sâ€²[1] == s[1]) / length(mg.hexes)
    else
        prob = mg.hexWorldDiscreteMDP.T[s[1], a[1], sâ€²[1]] * mg.hexWorldDiscreteMDP.T[s[2], a[2], sâ€²[2]]
    end

    return prob
end

# calculate reward for agent i in state s
function reward(mg::PredatorPreyHexWorldMG, i::Int, s, a)
    r = 0.0

    if i == 1
        # Predator get -1 for moving and 10 for catching the prey.
        if s[1] == s[2]
            return 10.0
        else
            return -1.0
        end
    elseif i == 2
        # Prey get -1 for moving and -100 for being caught.
        if s[1] == s[2]
            r = -100.0
        else
            r = -1.0
        end
    end

    return r
end

# calculate the rewards of 2 agents in state s
function joint_reward(mg::PredatorPreyHexWorldMG, s, a)
    return [reward(mg, i, s, a) for i in 1:n_agents(mg)]
end


# initialize MG from PredatorPreyHexWorldMG
function MG(mg::PredatorPreyHexWorldMG)
    return MG(
        mg.hexWorldDiscreteMDP.Î³,
        vec(collect(1:n_agents(mg))),
        ordered_states(mg),
        [ordered_actions(mg, i) for i in 1:n_agents(mg)],
        (s, a, sâ€²) -> transition(mg, s, a, sâ€²),
        (s, a) -> joint_reward(mg, s, a)
    )
end

# initialize PredatorPreyHexWorldMG from HexWorldMDP
function PredatorPreyHexWorldMG(hexes::Vector{Tuple{Int,Int}},
    r_bump_border::Float64,
    p_intended::Float64,
    Î³::Float64)
    hexWorld = HexWorldMDP(hexes,
        r_bump_border,
        p_intended,
        Dict{Tuple{Int64,Int64},Float64}(),
        Î³)
    mdp = hexWorld.mdp
    return PredatorPreyHexWorldMG(hexes, mdp)
end

# structure for visualization the result
struct VisualizePPHW
    model # calculate the probability of each action over each iteration
    policy # agents policy per iteration
    states # agents states per iteration
    rewards # agents rewards per iteration
    captured # save the iteration that the prey is captured
    function VisualizePPHW(k_max)
        k_max += 1
        model = [DataFrame(east = zeros(k_max), north_east = zeros(k_max), north_west = zeros(k_max), west = zeros(k_max), south_west = zeros(k_max), south_east = zeros(k_max)),
            DataFrame(east = zeros(k_max), north_east = zeros(k_max), north_west = zeros(k_max), west = zeros(k_max), south_west = zeros(k_max), south_east = zeros(k_max))]
        policy = [DataFrame(east = zeros(k_max), north_east = zeros(k_max), north_west = zeros(k_max), west = zeros(k_max), south_west = zeros(k_max), south_east = zeros(k_max)),
            DataFrame(east = zeros(k_max), north_east = zeros(k_max), north_west = zeros(k_max), west = zeros(k_max), south_west = zeros(k_max), south_east = zeros(k_max))]
        states = Vector{Tuple{Int64,Int64}}()
        rewards = Vector{Tuple{Int64,Int64}}()
        captured = Vector{Int}()

        # initialize for iteration 0
        push!(rewards, (0, 0))
        model[1][1, :] .= [1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6]
        model[2][1, :] .= [1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6]

        return new(model, policy, states, rewards, captured)
    end
end

# const HexWorldRBumpBorder = -1.0 # Reward for falling off hex map
# const HexWorldPIntended = 0.7 # Probability of going intended direction
# const HexWorldDiscountFactor = 0.9

function PredatorPreyHexWorld()
    PredatorPreyHexWorld = PredatorPreyHexWorldMG(
        [
            (-1, 2), (0, 2), (1, 2),
            (-1, 1), (1, 1), (3, 1), (4, 1),
            (0, 0), (1, 0), (2, 0), (3, 0), (4, 0),
        ],
        HexWorldRBumpBorder,    # we don't use this
        HexWorldPIntended,
        HexWorldDiscountFactor
    )
    return PredatorPreyHexWorld
end


struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

# return probability agent i will do action ai in state s
(Ï€i::MGPolicy)(s, ai) = Ï€i.p[s](ai)

# return probability agent i will do action ai
(Ï€i::SimpleGamePolicy)(s, ai) = Ï€i(ai)

# Ï€: MGPolicy
# return probability 2 agents will do action a in state s
probability(ð’«::MG, s, Ï€, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))

# evaluate reward of agent i in state s = reward (agent i in state s)* probability ??
reward(ð’«::MG, s, Ï€, i) =
    sum(ð’«.R(s, a)[i] * probability(ð’«, s, Ï€, a) for a in joint(ð’«.ð’œ))

# sum of probabilities that 2 agents can switch from s to s'
transition(ð’«::MG, s, Ï€, sâ€²) =
    sum(ð’«.T(s, a, sâ€²) * probability(ð’«, s, Ï€, a) for a in joint(ð’«.ð’œ))



mutable struct MGFictitiousPlay
    ð’« # Markov game
    i # agent index
    Qi # state-action value estimates
    Ni # state-action counts
end

function MGFictitiousPlay(ð’«::MG, i)
    â„, ð’®, ð’œ, R = ð’«.â„, ð’«.ð’®, ð’«.ð’œ, ð’«.R

    # Qi = [(s, a) => reward of agent i]
    Qi = Dict((s, a) => R(s, a)[i] for s in ð’® for a in joint(ð’œ))

    # initialize with all state-action counts = 1
    # Ni = [(agent j, state s, action aj) => 1]
    Ni = Dict((j, s, aj) => 1.0 for j in â„ for s in ð’® for aj in ð’œ[j])
    return MGFictitiousPlay(ð’«, i, Qi, Ni)
end


function (Ï€i::MGFictitiousPlay)(s)
    ð’«, i, Qi = Ï€i.ð’«, Ï€i.i, Ï€i.Qi

    # ð’«: Markov game
    â„, ð’®, ð’œ, T, R, Î³ = ð’«.â„, ð’«.ð’®, ð’«.ð’œ, ð’«.T, ð’«.R, ð’«.Î³

    # count the number of times each action has been performed at state s
    # return SimpleGamePolicy(Dict( action ai => Ni[i, s, ai]) )
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ð’œ[i])

    # MGPolicy(Dict{s => SimpleGamePolicy}
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ð’®)

    Ï€ = [Ï€iâ€²(i) for i in â„]

    # display(Ï€[1].p[(1,2)](4))

    # estimate the utility of agent i: state value estimates for s
    U(s, Ï€) = sum(Ï€i.Qi[s, a] * probability(ð’«, s, Ï€, a) for a in joint(ð’œ))

    # estimate the Q-value of agent i: reward of state s + state value estimates for the next state (s' - all possible states)
    Q(s, Ï€) = reward(ð’«, s, Ï€, i) + Î³ * sum(transition(ð’«, s, Ï€, sâ€²) * U(sâ€², Ï€)
                                           for sâ€² in ð’®)


    # joint : replace Ï€[i] with Ï€i
    # joint return [SimpleGamePolicy, MGPolicy ] or [MGPolicy, SimpleGamePolicy]
    # SimpleGamePolicy(Dict(ai => 1.0)), all actions different ai => 0

    # estimate the Q-value of agent i if it chooses action ai for the next turn (ai => 1.0)
    Q(ai) = Q(s, joint(Ï€, SimpleGamePolicy(ai), i))

    # return action that has the highest Q-value 
    ai = argmax(Q, ð’«.ð’œ[Ï€i.i])

    # return the policy with probability of action ai => 1.0 
    return SimpleGamePolicy(ai)
end

# update policy Ï€i after transform from s to s' with action a
function update!(Ï€i::MGFictitiousPlay, s, a, sâ€², v, iteration)
    ð’«, i, Qi = Ï€i.ð’«, Ï€i.i, Ï€i.Qi
    â„, ð’®, ð’œ, T, R, Î³ = ð’«.â„, ð’«.ð’®, ð’«.ð’œ, ð’«.T, ð’«.R, ð’«.Î³

    # update Ni
    for (j, aj) in enumerate(a)
        Ï€i.Ni[j, s, aj] += 1
    end

    # update visualize
    v.policy[i][iteration, a[i]] = 1

    totalCount = sum(Ï€i.Ni[i, S, ai] for S in ð’® for ai in ð’œ[i])
    for ai in ð’œ[i]
        v.model[i][iteration+1, ai] = sum(Ï€i.Ni[i, S, ai] for S in ð’®) / totalCount
    end

    # count the number of times each action has been performed at state s
    # return SimpleGamePolicy(Dict( action ai => Ni[i, s, ai]) )
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ð’œ[i])

    # MGPolicy(Dict{s => SimpleGamePolicy}
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ð’®)

    Ï€ = [Ï€iâ€²(i) for i in â„]

    # estimate the utility of agent i: state value estimates for s
    U(Ï€, s) = sum(Ï€i.Qi[s, a] * probability(ð’«, s, Ï€, a) for a in joint(ð’œ))

    # calculate the state-action value estimates of agent i: 
    # reward of state s + state value estimates for the next state s' when take action a
    Q(s, a) = R(s, a)[i] + Î³ * sum(T(s, a, sâ€²) * U(Ï€, sâ€²) for sâ€² in ð’®)

    # update Qi
    for a in joint(ð’œ)
        Ï€i.Qi[s, a] = Q(s, a)
    end
end


# random the next turn
function randstep(ð’«::MG, s, a)
    # random s' base on the distribution of the transition 
    sâ€² = rand(SetCategorical(ð’«.ð’®, [ð’«.T(s, a, sâ€²) for sâ€² in ð’«.ð’®]))

    # reward in state s
    r = ð’«.R(s, a)

    return sâ€², r
end


# random initial state of 2 agents
function randInitialState(ð’®)
    s = rand(ð’®)

    while s[1] == s[2]
        s = rand(ð’®)
    end
    return s
end

# simulate the joint policy Ï€ for k_max steps
function simulate(ð’«::MG, Ï€, k_max)

    v = VisualizePPHW(k_max)

    s = randInitialState(ð’«.ð’®)

    push!(v.states, s)
    for k = 1:k_max

        # choose action a for the next turn
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)

        # random state 
        sâ€², r = randstep(ð’«, s, a)
        # println(s," => ",sâ€²)
        for Ï€i in Ï€
            # update láº¡i policy
            update!(Ï€i, s, a, sâ€², v, k)
        end

        # update reward visualize

        # update reward = 0 if the agent does not move
        if (s[1] != s[2])
            if (s[1] == sâ€²[1])
                r[1] = 0
            end
            if (s[2] == sâ€²[2])
                r[2] = 0
            end
        else # the prey is captured
            push!(v.captured, k)
        end
        if (k > 1)
            r[1] += v.rewards[k][1]
            r[2] += v.rewards[k][2]
        end
        push!(v.rewards, Tuple(r))
        push!(v.states, sâ€²)

        # use sâ€² as current state for next iteration 
        s = sâ€²

    end

    # just for visualization purposes
    a = Tuple(Ï€i(s)() for Ï€i in Ï€)
    for i in 1:2
        v.policy[i][k_max+1, a[i]] = 1
    end

    return v, Ï€
end


#-------------- main ---------------

p = PredatorPreyHexWorld()
mg = MG(p)
Ï€ = [MGFictitiousPlay(mg, i) for i in 1:2]
k_max = 10

v, policy = simulate(mg, Ï€, k_max)


# choose 1 for 2 visualizations (uncomment from the comments starting with @@@)



# @@@ use to visualize step by step (limit k_max <= 10)
drawStepbyStepPredatorPreyHW(v.states, v.rewards, v.captured, k_max)


# @@@ used for general visualization
# visualizeGeneralPredatorPreyHW(v)

