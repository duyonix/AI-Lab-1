import Pkg;
Pkg.add("Parameters");
using Random
include("BabyPOMG.jl")
include("../helpers/POMG/ConditionalPlan.jl")
include("../helpers/POMG/POMG.jl")
include("../helpers/POMG/POMGDynamicProgramming.jl")
include("../helpers/POMG/POMGNashEquilibrium.jl")
include("../helpers/SimpleGame/SimpleGame.jl")
include("../helpers/SimpleGame/SimpleGamePolicy.jl")
include("../helpers/SimpleGame/NashEquilibrium.jl")
include("FormatAnswer.jl")

function MultiCaregiverCryingBaby()
    BabyPOMDP = CryingBaby()
    return BabyPOMG(BabyPOMDP)
end

function create_conditional_plans(ğ’«, d)
    # create all conditional plan with depth d from P::POMG
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
    end
    return Î 
end

function expand_conditional_plans(ğ’«, Î )
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i])) for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end



function solve(M::POMGNashEquilibrium, ğ’«::POMG)
    # step 1: convert POMG to SimpleGame
    # step 2: find Nash Equilibrium of that SimpleGame
    â„, Î³, b, d = ğ’«.â„, ğ’«.Î³, M.b, M.d
    Î  = create_conditional_plans(ğ’«, d)
    U = Dict(Ï€ => utility(ğ’«, b, Ï€) for Ï€ in joint(Î ))
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> U[Ï€])
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

function solve(M::POMGDynamicProgramming, ğ’«::POMG)
    â„, ğ’®, ğ’œ, R, Î³, b, d = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³, M.b, M.d
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
        prune_dominated!(Î , ğ’«) # after expand, prune dominated conditional plan
    end
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> utility(ğ’«, b, Ï€))
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

function prune_dominated!(Î , ğ’«::POMG)
    # prune any policy that is dominated by another policies
    done = false
    while !done
        done = true
        for i in shuffle(ğ’«.â„)
            for Ï€i in shuffle(Î [i])
                if length(Î [i]) > 1 && is_dominated(ğ’«, Î , i, Ï€i)
                    filter!(Ï€iâ€² -> Ï€iâ€² â‰  Ï€i, Î [i])
                    done = false
                    break
                end
            end
        end
    end
end

function is_dominated(ğ’«::POMG, Î , i, Ï€i)
    # check if policy is dominated
    â„, ğ’® = ğ’«.â„, ğ’«.ğ’®
    jointÎ noti = joint([Î [j] for j in â„ if j â‰  i])
    Ï€(Ï€iâ€², Ï€noti) = [j == i ? Ï€iâ€² : Ï€noti[j > i ? j - 1 : j] for j in â„]
    Ui = Dict((Ï€iâ€², Ï€noti, s) => evaluate_plan(ğ’«, Ï€(Ï€iâ€², Ï€noti), s)[i]
              for Ï€iâ€² in Î [i], Ï€noti in jointÎ noti, s in ğ’®)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î´)
    @variable(model, b[jointÎ noti, ğ’®] â‰¥ 0)
    @objective(model, Max, Î´)
    @constraint(model, [Ï€iâ€² = Î [i]],
        sum(b[Ï€noti, s] * (Ui[Ï€iâ€², Ï€noti, s] - Ui[Ï€i, Ï€noti, s])
            for Ï€noti in jointÎ noti for s in ğ’®) â‰¥ Î´)
    @constraint(model, sum(b) == 1)
    optimize!(model)
    return value(Î´) â‰¥ 0
end


b = [0.5, 0.5] # initial state distribution, b[sated]=b[hungry]=0.5, we can set this to [0.2, 0.8]
d = 3 # depth of conditional plans

# pomgDP=POMGDynamicProgramming(b, 5) # uncomment 2 lines and comment 2 lines to run POMGDP
pomgNash=POMGNashEquilibrium(b, d)
# ans=solve(pomgDP, pomg)
ans = solve(pomgNash, pomg)
printAns(ans)

